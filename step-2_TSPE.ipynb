{"cells":[{"cell_type":"markdown","id":"U2eOtjHV2HfK","metadata":{"id":"U2eOtjHV2HfK"},"source":["# Notes"]},{"cell_type":"markdown","id":"Is8EaEHWhXuz","metadata":{"id":"Is8EaEHWhXuz"},"source":["Issues:\n","1. ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (20, 2) + inhomogeneous part.\n","\n","This may due to the numpy version.\n","e.g. 1.25.2 has such issue; 1.23.1 works\n","\n","!pip install numpy==1.23.1\n","\n","link: https://stackoverflow.com/questions/67183501/setting-an-array-element-with-a-sequence-requested-array-has-an-inhomogeneous-sh\n"]},{"cell_type":"markdown","id":"XMZ06nZzOEku","metadata":{"id":"XMZ06nZzOEku"},"source":["# XQ:Select GPU"]},{"cell_type":"markdown","id":"wnx5TUIEOMXX","metadata":{"id":"wnx5TUIEOMXX"},"source":["Runtime -> Change Runtime type -> T4 GPU"]},{"cell_type":"markdown","id":"JHNfn4MVN9Fy","metadata":{"id":"JHNfn4MVN9Fy"},"source":["#XQ: check nvidia"]},{"cell_type":"markdown","id":"eyQbCqXNAIP8","metadata":{"id":"eyQbCqXNAIP8"},"source":["## install if needed"]},{"cell_type":"code","execution_count":null,"id":"KBm49WjMX3FG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBm49WjMX3FG","outputId":"f00f737f-1e46-448c-b396-2cc97ab42c5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nvidia-pyindex\n","  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: nvidia-pyindex\n","  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8419 sha256=9c77ce6b59f64da924cb407482ae7f84ba351c73c8f2edc6c8ad4efc81f532f6\n","  Stored in directory: /root/.cache/pip/wheels/2c/af/d0/7a12f82cab69f65d51107f48bcd6179e29b9a69a90546332b3\n","Successfully built nvidia-pyindex\n","Installing collected packages: nvidia-pyindex\n","Successfully installed nvidia-pyindex-1.0.9\n"]}],"source":["!pip install --upgrade nvidia-pyindex"]},{"cell_type":"markdown","id":"OZ09R7tlAK4g","metadata":{"id":"OZ09R7tlAK4g"},"source":["## check"]},{"cell_type":"code","execution_count":null,"id":"yGG2Z3u4XSIb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGG2Z3u4XSIb","outputId":"ceb35978-fbde-4657-c54c-451d1a161904"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Jun 17 05:17:23 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","id":"Q6Vf5O0JOu31","metadata":{"id":"Q6Vf5O0JOu31"},"source":["# Packages"]},{"cell_type":"markdown","id":"GeVa1Pf_0lAa","metadata":{"id":"GeVa1Pf_0lAa"},"source":["## XQ: pip install"]},{"cell_type":"markdown","id":"MD11y7kziqwO","metadata":{"id":"MD11y7kziqwO"},"source":["### For numpy version issue, no need if this issue is solved"]},{"cell_type":"code","execution_count":null,"id":"SXms95Q1iV8_","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":526},"id":"SXms95Q1iV8_","outputId":"dcc8ac95-beeb-43a7-f982-6e83ac95d36b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.23.1\n","  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n","Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n","albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n","bigframes 1.24.0 requires numpy>=1.24.0, but you have numpy 1.23.1 which is incompatible.\n","chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n","ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.23.1 which is incompatible.\n","jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n","jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n","mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n","pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n","plotnine 0.14.0 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n","tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 1.23.1 which is incompatible.\n","xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"99e6109c4d4d4391bd4e8099327621a1"}},"metadata":{}}],"source":["!pip install numpy==1.23.1"]},{"cell_type":"code","execution_count":null,"id":"s0D3ooJ2ihWv","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"s0D3ooJ2ihWv","outputId":"07118d63-9587-413f-8f8b-bb57ea83d145"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.23.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["import numpy as np\n","np.version.version"]},{"cell_type":"markdown","id":"nNlCo9yxixGo","metadata":{"id":"nNlCo9yxixGo"},"source":["### For notebook import"]},{"cell_type":"code","execution_count":null,"id":"n6FrARnyFiUg","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6FrARnyFiUg","outputId":"7ef07932-d9a8-45b4-e8b5-37a36b6575bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting import_ipynb\n","  Downloading import_ipynb-0.2-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (7.34.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (5.10.4)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (75.1.0)\n","Collecting jedi>=0.16 (from IPython->import_ipynb)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (3.0.48)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.9.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (5.7.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.20.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (4.3.6)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.13)\n","Downloading import_ipynb-0.2-py3-none-any.whl (4.0 kB)\n","Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jedi, import_ipynb\n","Successfully installed import_ipynb-0.2 jedi-0.19.1\n"]}],"source":["!pip install import_ipynb"]},{"cell_type":"markdown","id":"0quiQZnHBMnE","metadata":{"id":"0quiQZnHBMnE"},"source":["#Mount Drive"]},{"cell_type":"code","execution_count":null,"id":"zLEqL7d4O4em","metadata":{"id":"zLEqL7d4O4em","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a404b327-509d-4337-847b-ed8c5dd3cda6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# import packages\n","## for mount drive purpose\n","import os\n","from google.colab import drive\n","# mount drive\n","drive.mount('/content/drive/', force_remount=True)\n","os.chdir('/content/drive/My Drive/Colab_Notebooks/transformerGO/TransformerGO_RR/')"]},{"cell_type":"markdown","id":"2a8qpRit4urW","metadata":{"id":"2a8qpRit4urW"},"source":["# packages"]},{"cell_type":"code","execution_count":null,"id":"QD8XJYEC4FBQ","metadata":{"id":"QD8XJYEC4FBQ"},"outputs":[],"source":["from utils import get_gene_idx_dict_from_file, file_to_matrix\n","import pickle\n","import numpy as np\n","\n","import torch\n","import torch.utils.data as data\n","\n","import import_ipynb\n","from training_helper import transformerGO_collate_fn\n","\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn import metrics\n","\n","import numpy as np\n","import time\n","\n","from tqdm import tqdm as prog_bar #The progress bar\n","\n","import import_ipynb\n","from transformerGO import (TransformerGO_Scratch)\n","from training_helper import (print_status, write_scalars_tensorboard,\n","                             binary_accuracy, epoch_time)"]},{"cell_type":"markdown","id":"oC7b3SKbAX7-","metadata":{"id":"oC7b3SKbAX7-"},"source":["# set info\n"]},{"cell_type":"code","execution_count":null,"id":"ucbM10J-4d_F","metadata":{"id":"ucbM10J-4d_F"},"outputs":[],"source":["input_folder = 'input'\n","dataset = 'RR1'\n","emb_folder = \"embedding\"\n","\n","\n","# PSE = 'NoPE' # opt0\n","# PSE = 'LPE' # opt1\n","PSE = 'SPE' # opt2"]},{"cell_type":"markdown","id":"tr6jGxJe4zKu","metadata":{"id":"tr6jGxJe4zKu"},"source":["# functions"]},{"cell_type":"code","execution_count":null,"id":"4IQETJLY4xSh","metadata":{"id":"4IQETJLY4xSh"},"outputs":[],"source":["# Mine\n","def get_disease_sets(file_path):\n","    dis_pairs = []   #[(disA, disB), ...]\n","    labels = []      # [label, ...]\n","    disease_genes_dict = {}     #{disease: [gene_1, gene_2, ...]}\n","\n","    f = open(file_path, \"r\")\n","    head = True\n","    for line in f:\n","        if head:\n","            head = False\n","            continue\n","\n","        row = line.strip().split(\"\\t\")\n","        dis_pair, disease_a_genes, disease_b_genes, all_genes, rr = row\n","\n","        disease_a, disease_b = dis_pair.split(\"&\")\n","\n","        dis_pairs.append((disease_a, disease_b))\n","        labels.append(int(rr))\n","\n","        disease_genes_dict[disease_a] = disease_a_genes.split(\",\")\n","        disease_genes_dict[disease_b] = disease_b_genes.split(\",\")\n","\n","\n","    f.close()\n","\n","    return dis_pairs, labels, disease_genes_dict\n","\n","\n","class Dataset_torch(torch.utils.data.Dataset):\n","    #Characterizes a dataset for PyTorch\n","    def __init__(self, dis_pairs, labels, dis_genes_dict, node_idx_dict, emb, lpe, combine_opt):\n","        self.dis_pairs = dis_pairs\n","        self.labels = labels\n","        self.dis_genes_dict = dis_genes_dict\n","        self.node_idx_dict = node_idx_dict\n","        self.emb = emb\n","        self.lpe = lpe\n","        self.opt = combine_opt\n","\n","    def __len__(self):\n","        return len(self.dis_pairs)\n","\n","    def __getitem__(self, index):\n","\n","        label = self.labels[index]\n","        disA,disB = self.dis_pairs[index]\n","        gene_lists = [self.dis_genes_dict[disA], self.dis_genes_dict[disB]]\n","        dis_pair_genes = [(disA, gene_lists[0]), (disB, gene_lists[1])] # [(disA, disA_gene_list), (disB, disB_gene_list)]\n","        features = [get_features(gene_list, node_idx_dict, self.emb, self.lpe, self.opt) for gene_list in gene_lists]\n","\n","        return np.array((features, label, dis_pair_genes), dtype=object)\n","\n","#------------------------------------------------------------------------------#\n","def get_features(gene_list, node_idx_dict, emb, lpe, combine_opt):\n","    node_idices = [node_idx_dict[gene] for gene in gene_list if gene in node_idx_dict]\n","    feature_vecs = emb[node_idices, :]\n","    if lpe is not None:\n","      if combine_opt == \"add\":\n","        feature_vecs = np.add(emb[node_idices, :], lpe[node_idices, :])\n","      else:\n","        feature_vecs = np.concatenate((emb[node_idices, :], lpe[node_idices, :]), axis=1)\n","\n","    return feature_vecs\n","\n","#------------------------------------------------------------------------------#\n","def split_train_valid(ori_dataset, ratio):\n","  sz = len(ori_dataset)\n","  train_set, valid_set = data.random_split(ori_dataset, [int(ratio[0]*sz), sz - (int(ratio[0]*sz)) ] )\n","\n","  return train_set, valid_set\n","\n","########################TransformerGO###########################################\n","def get_max_len_seq(dataset):\n","    \"\"\"Finds the dis with the most genes and returns the size\"\"\"\n","    batch_features, batch_labels, batch_ids  = zip(*dataset)\n","    batch_features = np.array(batch_features)\n","\n","\n","    max_len = 0\n","    for i in range(0, batch_features.shape[0]):\n","        max_len = max(max_len, len(batch_features[i][0]), len(batch_features[i][1]))\n","    return max_len\n","\n","def helper_collate(batch):\n","    MAX_LEN_SEQ = get_max_len_seq(batch)\n","    return transformerGO_collate_fn(batch, MAX_LEN_SEQ, EMB_DIM, pytorch_pad = False)\n","\n","def train(model, iterator, optimizer, criterion,  torch_vers = False):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.train()\n","\n","    pred = []\n","    lab = []\n","    for batch in prog_bar(iterator):\n","        optimizer.zero_grad()\n","\n","        #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","        padded_pairs = batch[0].to(device)\n","        labels = batch[1].to(device)\n","        mask = batch[2].to(device)\n","\n","        #split data into protA and protB\n","        gosetA_batch = padded_pairs[:,0]\n","        gosetB_batch = padded_pairs[:,1]\n","\n","        #permute the data to fit the pytorch transformer\n","        if torch_vers:\n","            gosetA_batch = gosetA_batch.permute(1,0,2)\n","            gosetB_batch = gosetB_batch.permute(1,0,2)\n","\n","        predictions = model(gosetA_batch, gosetB_batch, mask[:,0], mask[:,1]).squeeze(1)\n","        loss = criterion(predictions, labels)\n","        acc = binary_accuracy(predictions, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        pred = pred + list(predictions.cpu().data.numpy())\n","        lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred)\n","\n","def evaluate(model, iterator, criterion, torch_vers = False):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    pred = []\n","    lab = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","\n","            #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","            padded_pairs = batch[0].to(device)\n","            labels = batch[1].to(device)\n","            mask = batch[2].to(device)\n","\n","            #split data into protA and protB\n","            gosetA_batch = padded_pairs[:,0]\n","            gosetB_batch = padded_pairs[:,1]\n","\n","            #permute the data to fit the pytorch transformer\n","            if torch_vers:\n","                gosetA_batch = gosetA_batch.permute(1,0,2)\n","                gosetB_batch = gosetB_batch.permute(1,0,2)\n","\n","            predictions = model(gosetA_batch, gosetB_batch, mask[:,0], mask[:,1]).squeeze(1)\n","            loss = criterion(predictions, labels)\n","            acc = binary_accuracy(predictions, labels)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","            pred = pred + list(predictions.cpu().data.numpy())\n","            lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred), lab, pred\n","\n","def evaluate_cosine(iterator, criterion, torch_vers = False):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    pred = []\n","    lab = []\n","\n","    for batch in prog_bar(iterator):\n","\n","        #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","        padded_pairs = batch[0].to(device)\n","        labels = batch[1].to(device)\n","\n","        #split data into protA and protB\n","        gosetA_batch = padded_pairs[:,0]\n","        gosetB_batch = padded_pairs[:,1]\n","\n","        vector_protA = torch.sum(gosetA_batch, dim = 1)\n","        vector_protB = torch.sum(gosetB_batch, dim = 1)\n","\n","        cosine = nn.CosineSimilarity(dim=1, eps=1e-6)\n","        predictions = cosine(vector_protA, vector_protB)\n","        loss = criterion(predictions, labels)\n","        acc = binary_accuracy(predictions, labels)\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        pred = pred + list(predictions.cpu().data.numpy())\n","        lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), lab, pred"]},{"cell_type":"markdown","id":"D3AXPld2_u2u","metadata":{"id":"D3AXPld2_u2u"},"source":["# Main"]},{"cell_type":"code","execution_count":null,"id":"h56muLpm6brP","metadata":{"id":"h56muLpm6brP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a7d9e061-1ca3-4443-990c-ff65d8406963"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train set:  8702 \n"," Valid set:  967 \n"," Test set:  1074 \n","\n","emb dim: 72\n","Device available:  cuda   Tesla T4\n","442729\n"]}],"source":["node_file_path = f'{input_folder}/interactom_nodes.txt'   # stores the nodes for the largest connected component in human Interactome\n","train_file_path = f'{input_folder}/{dataset}/{fold}/train_set.tsv'\n","test_file_path = f'{input_folder}/{dataset}/{fold}/test_set.tsv'\n","\n","\n","# 1. get graph original nodes\n","node_idx_dict = get_gene_idx_dict_from_file(node_file_path)\n","node_gene_dict = {v:k for k,v in node_idx_dict.items()}\n","\n","# 2. get selected disease pairs\n","train_dis_pairs, train_labels, train_disease_genes_dict = get_disease_sets(train_file_path)\n","test_dis_pairs, test_labels, test_disease_genes_dict = get_disease_sets(test_file_path)\n","\n","# 3. get embedding file\n","#### opt0: n2v from graph with id mapping, 64 d\n","if PSE == 'NoPE':\n","  emb_dim = 64\n","  lpe_dim = 0\n","  emb_file = f'{emb_folder}/node2nev_emb_64_for_PE' # f'{emb_folder}/node2nev_emb_64'\n","  with open(emb_file, 'rb') as f:\n","      emb = pickle.load(f)\n","  lpe = None\n","  combine_opt = None\n","\n","#### opt1: n2v from graph with id mapping + LPE Add\n","if PSE == 'LPE':\n","  emb_dim = 64\n","  lpe_dim = 64\n","  emb_file = f'{emb_folder}/node2nev_emb_64_for_PE'\n","  with open(emb_file, 'rb') as f:\n","      emb = pickle.load(f)\n","\n","  lpe_file = f'{emb_folder}/LPE.tsv'\n","  lpe = file_to_matrix(lpe_file)\n","  combine_opt = \"add\"\n","\n","\n","# #### opt2: n2v from graph with id mapping + LPE add + GPE concat\n","if PSE == 'SPE':\n","  emb_dim = 64\n","  lpe_dim = 64\n","\n","  emb_file = f'{emb_folder}/node2nev_emb_64_for_PE'\n","  with open(emb_file, 'rb') as f:\n","      emb = pickle.load(f)\n","\n","  # --add lpe\n","  lpe_file = f'{emb_folder}/LPE.tsv'\n","  lpe = file_to_matrix(lpe_file)\n","  emb = np.add(emb, lpe)\n","\n","  # --concat gpe\n","  lpe_dim = 8\n","\n","  lpe_file = f'{emb_folder}/{dataset}/GEE_Z_U.tsv'\n","  lpe = file_to_matrix(lpe_file)\n","  lpe = lpe[:,1:lpe_dim+1]\n","  combine_opt = \"concat\"\n","\n","\n","train_origin_set = Dataset_torch(train_dis_pairs, train_labels, train_disease_genes_dict, node_idx_dict, emb, lpe, combine_opt)\n","ratio = [0.9, 0.1] # [0.8, 0.2]  # [0.9, 0.1]\n","train_set, valid_set = split_train_valid(train_origin_set, ratio)\n","test_set = Dataset_torch(test_dis_pairs, test_labels, test_disease_genes_dict, node_idx_dict, emb,  lpe, combine_opt)\n","\n","params = {'batch_size': 20,'collate_fn': helper_collate}\n","\n","print(\"Train set: \", len(train_set), '\\n', \"Valid set: \", len(valid_set), '\\n', \"Test set: \", len(test_set), '\\n')\n","train_grt = data.DataLoader(train_set, **params, shuffle = True)\n","val_grt = data.DataLoader(valid_set, **params, shuffle = True)\n","test_grt = data.DataLoader(test_set, **params, shuffle = False)\n","\n","\n","if lpe_dim == 64:\n","  EMB_DIM = emb_dim\n","else: EMB_DIM = emb_dim + lpe_dim  #N2V + LPE, 64 *2 dimensions too much, cannot run\n","\n","print(f\"emb dim: {EMB_DIM}\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device available: \", device, \" \", torch.cuda.get_device_name(0))\n","\n","#TRANSFORMERGO MODEL #\n","MODEL_SIZE = EMB_DIM\n","NR_HEADS = 8\n","NR_LAYERS = 3\n","DROPOUT = 0.2\n","SIZE_FF = 4 * MODEL_SIZE\n","LR = 0.0001\n","\n","# if see AssertionError, this is because the dimension (EMB_DIM) cannot divide the headnumber (NR_HEADS)\n","# for the current one, add 8 dim for lpe and keep original 64 dims\n","model = TransformerGO_Scratch(MODEL_SIZE, NR_HEADS, NR_LAYERS, SIZE_FF, DROPOUT)\n","\n","model = model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.BCEWithLogitsLoss().to(device) # this is the activation function used\n","\n","pytorch_total_params = sum(p.numel() for p in model.parameters())"]},{"cell_type":"code","source":["model_name = \"model.pt\"\n","\n","writer = SummaryWriter(flush_secs=14)\n","N_EPOCHS = 30\n","best_roc_val = float('-inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","\n","    start_time = time.time()\n","    train_loss, train_acc, roc_train = train(model, train_grt, optimizer, criterion, torch_vers = False)\n","    valid_loss, valid_acc, roc_val, _, _ = evaluate(model, val_grt, criterion, torch_vers = False)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if best_roc_val < roc_val:\n","        best_roc_val = roc_val\n","        torch.save(model.state_dict(),  model_name)\n","\n","    print_status(epoch, epoch_mins, epoch_secs, train_loss,\\\n","                 train_acc, valid_loss, valid_acc, roc_train, roc_val, optimizer)\n","    write_scalars_tensorboard(writer, train_loss, valid_loss, train_acc, valid_acc, epoch)\n"],"metadata":{"id":"p-oCKqUA1yrS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8a46b1c8-0682-457e-a79f-ca9b0b8cb30f"},"id":"p-oCKqUA1yrS","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/436 [00:00<?, ?it/s]<ipython-input-6-27195776c4aa>:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  batch_features = np.array(batch_features)\n","<string>:52: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  7%|▋         | 31/436 [00:27<04:13,  1.60it/s]"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["eyQbCqXNAIP8","OZ09R7tlAK4g","0quiQZnHBMnE","2a8qpRit4urW"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}