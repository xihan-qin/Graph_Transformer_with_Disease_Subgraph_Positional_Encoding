{"cells":[{"cell_type":"markdown","id":"XMZ06nZzOEku","metadata":{"id":"XMZ06nZzOEku"},"source":["# Select GPU"]},{"cell_type":"markdown","id":"wnx5TUIEOMXX","metadata":{"id":"wnx5TUIEOMXX"},"source":["Runtime -\u003e Change Runtime type -\u003e T4 GPU"]},{"cell_type":"markdown","id":"JHNfn4MVN9Fy","metadata":{"id":"JHNfn4MVN9Fy"},"source":["# Check nvidia"]},{"cell_type":"markdown","id":"eyQbCqXNAIP8","metadata":{"id":"eyQbCqXNAIP8"},"source":["## install if needed"]},{"cell_type":"code","execution_count":null,"id":"KBm49WjMX3FG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBm49WjMX3FG","outputId":"f00f737f-1e46-448c-b396-2cc97ab42c5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nvidia-pyindex\n","  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: nvidia-pyindex\n","  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8419 sha256=9c77ce6b59f64da924cb407482ae7f84ba351c73c8f2edc6c8ad4efc81f532f6\n","  Stored in directory: /root/.cache/pip/wheels/2c/af/d0/7a12f82cab69f65d51107f48bcd6179e29b9a69a90546332b3\n","Successfully built nvidia-pyindex\n","Installing collected packages: nvidia-pyindex\n","Successfully installed nvidia-pyindex-1.0.9\n"]}],"source":["!pip install --upgrade nvidia-pyindex"]},{"cell_type":"markdown","id":"OZ09R7tlAK4g","metadata":{"id":"OZ09R7tlAK4g"},"source":["## check"]},{"cell_type":"code","execution_count":null,"id":"yGG2Z3u4XSIb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1733328976603,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"yGG2Z3u4XSIb","outputId":"44e5edad-3269-4014-f475-755405b220ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","id":"0quiQZnHBMnE","metadata":{"id":"0quiQZnHBMnE"},"source":["#Mount Drive"]},{"cell_type":"code","execution_count":1,"id":"zLEqL7d4O4em","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25223,"status":"ok","timestamp":1734020794237,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"zLEqL7d4O4em","outputId":"24838740-3019-4f9d-eb56-24a18e95c666"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["# import packages\n","## for mount drive purpose\n","import os\n","from google.colab import drive\n","# mount drive\n","drive.mount('/content/drive/', force_remount=True)\n","os.chdir('/content/drive/My Drive/Colab_Notebooks/Graph_transformer/')"]},{"cell_type":"markdown","id":"2a8qpRit4urW","metadata":{"id":"2a8qpRit4urW"},"source":["# Packages"]},{"cell_type":"code","execution_count":2,"id":"QD8XJYEC4FBQ","metadata":{"executionInfo":{"elapsed":12742,"status":"ok","timestamp":1734020806977,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"QD8XJYEC4FBQ"},"outputs":[],"source":["from utils import get_gene_idx_dict_from_file, file_to_matrix\n","import pickle\n","import numpy as np\n","\n","import torch\n","import torch.utils.data as data\n","\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn import metrics\n","\n","from tqdm import tqdm as prog_bar #The progress bar\n","import math, copy, time\n","import torch.nn.functional as F\n","\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"markdown","id":"VLd5M2L-XuFM","metadata":{"id":"VLd5M2L-XuFM"},"source":["# Graph Transformer Framework"]},{"cell_type":"markdown","id":"8Do9Ge0VWp-T","metadata":{"id":"8Do9Ge0VWp-T"},"source":["## Harvard Transformer"]},{"cell_type":"markdown","id":"Ry6LLuaSX3qS","metadata":{"id":"Ry6LLuaSX3qS"},"source":["We use code from Harvard Transformer for the basic building blocks in Transformer. Modified to fix Degree of Freedom issue in Forward."]},{"cell_type":"markdown","id":"SshbzZVfW4_8","metadata":{"id":"SshbzZVfW4_8"},"source":["\n","@inproceedings{opennmt, author = {Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush}, title = {OpenNMT: Open-Source Toolkit for Neural Machine Translation}, booktitle = {Proc. ACL}, year = {2017}, url = {https://doi.org/10.18653/v1/P17-4012}, doi = {10.18653/v1/P17-4012} }"]},{"cell_type":"code","execution_count":3,"id":"5Ypoux4-VBEO","metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"5Ypoux4-VBEO"},"outputs":[],"source":["def clones(module, N):\n","    \"Produce N identical layers.\"\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n","\n","\n","class LayerNorm(nn.Module):\n","    \"Construct a layernorm module (See citation for details).\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","      \"\"\"\n","        XQ: modified to fix DOF issue\n","      \"\"\"\n","      mean = x.mean(-1, keepdim=True)\n","      if x.numel() \u003e 1:\n","          std = x.std(-1, keepdim=True)  # Standard deviation along the last dimension with keepdim=True\n","      else:\n","          std = torch.tensor(0.0).to(x.device)  # If only one element, set std to 0.0 (ensure itâ€™s on the same device as x)\n","      return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","\n","class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","\n","class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, mask):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","class EncoderLayer(nn.Module):\n","    \"Encoder is made up of self-attn and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        \"Follow Figure 1 (left) for connections.\"\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)\n","\n","\n","class Decoder(nn.Module):\n","    \"Generic N layer decoder with masking.\"\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)\n","\n","class DecoderLayer(nn.Module):\n","    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn\n","        self.src_attn = src_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n","\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        \"Follow Figure 1 (right) for connections.\"\n","        m = memory\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        return self.sublayer[2](x, self.feed_forward)\n","\n","def attention(query, key, value, mask=None, dropout=None):\n","    \"Compute 'Scaled Dot Product Attention'\"\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn\n","\n","class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, query, key, value, mask=None):\n","        \"Implements Figure 2\"\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","\n","        # 1) Do all the linear projections in batch from d_model =\u003e h x d_k\n","        # Zip only goes through the first 3 Layers - Ioan\n","        # Each matrix multiplications is done once and then split in heads\n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","\n","        # 2) Apply attention on all the projected vectors in batch.\n","        x, self.attn = attention(query, key, value, mask=mask,\n","                                 dropout=self.dropout)\n","\n","        # 3) \"Concat\" using a view and apply a final linear.\n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)\n","\n","        return self.linears[-1](x)\n","\n","class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"]},{"cell_type":"markdown","id":"lv53pmRIYanE","metadata":{"id":"lv53pmRIYanE"},"source":["## TransformerGO \u0026 TransformerCPI\n"]},{"cell_type":"markdown","id":"fBQgGYe3ZXFM","metadata":{"id":"fBQgGYe3ZXFM"},"source":["No positonal encoding + no mask in Decoder + Binary Classification\n","-- Methods and utils adopted from TransformerGO and TransformerCPI, then modified for our case"]},{"cell_type":"code","execution_count":4,"id":"0ZTabRONQ5VL","metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"0ZTabRONQ5VL"},"outputs":[],"source":["class TransformerDis(nn.Module):\n","    \"\"\"\n","      disA: Protein node set related to disease A\n","      disB: Protein node set related to disease B\n","    \"\"\"\n","    def __init__(self ,d_model, nhead, num_layers, dim_feedforward, dropout = 0.1):\n","        super().__init__()\n","\n","        c = copy.deepcopy\n","        attn = MultiHeadedAttention(nhead, d_model, dropout)\n","        ff = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n","\n","        self.encoder = Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), num_layers)\n","        self.decoder = Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), num_layers)\n","\n","        self.linear = nn.Linear(d_model, 1)\n","\n","    #batch  * max_seq_len * node2vec_dim\n","    def forward(self, emb_disA, emb_disB, disA_mask, disB_mask):\n","\n","        memory = self.encoder(emb_disA, disA_mask)\n","        output = self.decoder(emb_disB, memory, disA_mask, disB_mask)\n","        #output: batch * seqLen * embDim\n","\n","        #transform B * seqLen * node2vec_dim --\u003e B * node2vec_dim (TransformerCPI paper)\n","        output_c = torch.linalg.norm(output, dim = 2)\n","        output_c = F.softmax(output_c, dim = 1).unsqueeze(1)\n","        output = torch.bmm(output_c, output)\n","\n","        return self.linear(output).squeeze(1)\n"]},{"cell_type":"code","execution_count":5,"id":"MGc2dtIEa_zZ","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"MGc2dtIEa_zZ"},"outputs":[],"source":["def transformerGO_collate_fn(batch, max_size_set, emb_size = 64, pytorch_pad = False):\n","\n","    \"\"\" Function that remodels each batch of data before\n","    input to the transformer model\n","\n","    Args:\n","        batch (tuple):  #batch_features:  Shape N * ( [disALen, (emb+pos)Dim], [disBLen, (emb+pos)Dim] )\n","                        #batch_ids:       Shape N * ( [1, disALen], [1, disALen] )\n","                        #batch_labels:    Shape N * [1 or 0]\n","\n","    Returns:\n","    tensor: padded embedding+positional_encoding of shape N * 2(disease pair) * L(longest seq of pro related to one disease) * (emb+pos)Dim\n","    tensor: batch labels of   shape N * [1 or 0]\n","    tensor: padding of        shape N * 2 * L * L\n","    \"\"\"\n","\n","    batch_features, batch_labels, batch_ids  = zip(*batch)\n","    unpadded_seqs = []\n","    padd_mask_pytorch = torch.ones((len(batch_features), 2, max_size_set), dtype=torch.bool)\n","    padd_mask = torch.empty((len(batch_features), 2, max_size_set, max_size_set))\n","\n","    for i in range(0, len(batch_features)):\n","        protA = batch_features[i][0]\n","        protB = batch_features[i][1]\n","        unpadded_seqs.append( torch.FloatTensor(protA) )\n","        unpadded_seqs.append( torch.FloatTensor(protB) )\n","\n","        #mask those positions which are not padding\n","        padd_mask_pytorch[i][0][0:len(protA)] = False\n","        padd_mask_pytorch[i][1][0:len(protB)] = False\n","\n","    #pad embedings according to the largest in the entire dataset\n","    unpadded_seqs.append(torch.zeros(max_size_set, emb_size))\n","    padded_seq = pad_sequence(unpadded_seqs, batch_first = True)[:-1]\n","\n","    #create new tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","    s = padded_seq.shape\n","    padded_pairs = torch.empty((len(batch_features), 2, s[1], s[2]))\n","    padded_pairs[:,0] = padded_seq[0::2]\n","    padded_pairs[:,1] = padded_seq[1::2]\n","\n","    for i in range(0, padded_pairs.shape[0]):\n","        padd_mask[i][0] = get_padd_mask_transformer(padded_pairs[i][0])\n","        padd_mask[i][1] = get_padd_mask_transformer(padded_pairs[i][1])\n","\n","    if pytorch_pad:\n","        return padded_pairs, torch.FloatTensor(batch_labels), padd_mask_pytorch\n","\n","    return padded_pairs, torch.FloatTensor(batch_labels), padd_mask"]},{"cell_type":"code","execution_count":6,"id":"I8i-jZNDbRFh","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"I8i-jZNDbRFh"},"outputs":[],"source":["def get_padd_mask_transformer(emb):\n","    \"\"\"Gets an embedding matrix and returns its padding mask\n","    Args:\n","        emb (numpy): numpy of shape (seqLen, emb_dim)\n","\n","    Returns:\n","    numpy: matrix of size (seqLen, seqLen)\n","    \"\"\"\n","    mask = (emb.numpy() != 0)\n","    mask = np.matmul(mask, mask.T)\n","    return torch.from_numpy(mask)"]},{"cell_type":"code","execution_count":7,"id":"sopGLEI1behC","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"sopGLEI1behC"},"outputs":[],"source":["def binary_accuracy(preds, y):\n","    \"\"\"\n","    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n","    \"\"\"\n","    #round predictions to the closest integer\n","    rounded_preds = torch.round(torch.sigmoid(preds))\n","    correct = (rounded_preds == y).float() #convert into float for division\n","    acc = correct.sum() / len(correct)\n","    return acc"]},{"cell_type":"code","execution_count":8,"id":"AVEw59YyfNmx","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"AVEw59YyfNmx"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"]},{"cell_type":"code","execution_count":9,"id":"64bmRBnSbmRc","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"64bmRBnSbmRc"},"outputs":[],"source":["def print_status(epoch, epoch_mins, epoch_secs, train_loss, train_acc, valid_loss, valid_acc, roc_train, roc_val, optimizer):\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s'  ,\n","    f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%' ,\n","    f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%' ,\n","    f'\\t Roc Train: {roc_train:.3f}' , f'\\t Roc Valid: {roc_val:.3f}' ,\n","    \",  \", optimizer.param_groups[0]['lr'], \"--LR\", end='\\r')"]},{"cell_type":"code","execution_count":10,"id":"OQBA9f8eborO","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"OQBA9f8eborO"},"outputs":[],"source":["def write_scalars_tensorboard(writer, train_loss, valid_loss, train_acc, valid_acc, epoch):\n","    writer.add_scalars('Loss', {'train':train_loss, 'valid': valid_loss}, epoch)\n","    writer.add_scalars('Acc', {'train':train_acc, 'valid': valid_acc}, epoch)"]},{"cell_type":"markdown","id":"oC7b3SKbAX7-","metadata":{"id":"oC7b3SKbAX7-"},"source":["# Set info\n"]},{"cell_type":"code","execution_count":11,"id":"ucbM10J-4d_F","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"ucbM10J-4d_F"},"outputs":[],"source":["input_folder = 'input'\n","dataset = 'RR1' # 'RR0'\n","emb_folder = \"embedding\"\n","\n","\n","# PSE = 'NoPE' # opt0\n","PSE = 'LPE' # opt1\n","# PSE = 'SPE' # opt2"]},{"cell_type":"markdown","id":"tr6jGxJe4zKu","metadata":{"id":"tr6jGxJe4zKu"},"source":["# Functions"]},{"cell_type":"code","execution_count":12,"id":"4IQETJLY4xSh","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1734020806978,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"4IQETJLY4xSh"},"outputs":[],"source":["########################PrepData################################################\n","def get_disease_sets(file_path):\n","    dis_pairs = []   #[(disA, disB), ...]\n","    labels = []      # [label, ...]\n","    disease_genes_dict = {}     #{disease: [gene_1, gene_2, ...]}\n","\n","    f = open(file_path, \"r\")\n","    head = True\n","    for line in f:\n","        if head:\n","            head = False\n","            continue\n","\n","        row = line.strip().split(\"\\t\")\n","        dis_pair, disease_a_genes, disease_b_genes, all_genes, rr = row\n","\n","        disease_a, disease_b = dis_pair.split(\"\u0026\")\n","\n","        dis_pairs.append((disease_a, disease_b))\n","        labels.append(int(rr))\n","\n","        disease_genes_dict[disease_a] = [int(gene) for gene in disease_a_genes.split(\",\")]\n","        disease_genes_dict[disease_b] = [int(gene) for gene in disease_b_genes.split(\",\")]\n","\n","\n","    f.close()\n","\n","    return dis_pairs, labels, disease_genes_dict\n","\n","\n","class Dataset_torch(torch.utils.data.Dataset):\n","    #Characterizes a dataset for PyTorch\n","    def __init__(self, dis_pairs, labels, dis_genes_dict, node_idx_dict, emb, lpe, combine_opt):\n","        self.dis_pairs = dis_pairs\n","        self.labels = labels\n","        self.dis_genes_dict = dis_genes_dict\n","        self.node_idx_dict = node_idx_dict\n","        self.emb = emb\n","        self.lpe = lpe\n","        self.opt = combine_opt\n","\n","    def __len__(self):\n","        return len(self.dis_pairs)\n","\n","    def __getitem__(self, index):\n","        label = self.labels[index]\n","        disA,disB = self.dis_pairs[index]\n","        gene_lists = [self.dis_genes_dict[disA], self.dis_genes_dict[disB]]\n","        dis_pair_genes = [(disA, gene_lists[0]), (disB, gene_lists[1])] # [(disA, disA_gene_list), (disB, disB_gene_list)]\n","        features = [get_features(gene_list, node_idx_dict, self.emb, self.lpe, self.opt) for gene_list in gene_lists]\n","\n","        return np.array((features, label, dis_pair_genes), dtype=object)\n","\n","#------------------------------------------------------------------------------#\n","def get_features(gene_list, node_idx_dict, emb, lpe, combine_opt):\n","    # those keys are strings\n","    node_idices = [node_idx_dict[str(gene)] for gene in gene_list if str(gene) in node_idx_dict]\n","    feature_vecs = emb[node_idices, :]\n","    if lpe is not None:\n","      if combine_opt == \"add\":\n","        feature_vecs = np.add(emb[node_idices, :], lpe[node_idices, :])\n","      else:\n","        feature_vecs = np.concatenate((emb[node_idices, :], lpe[node_idices, :]), axis=1)\n","\n","    return feature_vecs\n","\n","#------------------------------------------------------------------------------#\n","def split_train_valid(ori_dataset, ratio):\n","  sz = len(ori_dataset)\n","  train_set, valid_set = data.random_split(ori_dataset, [int(ratio[0]*sz), sz - (int(ratio[0]*sz)) ] )\n","\n","  return train_set, valid_set\n","\n","#------------------------------------------------------------------------------#\n","def getEmbeddingAndVars(PSE):\n","  \"\"\"\n","    get embedding and positional encoding\n","    set var value based on given PSE\n","  \"\"\"\n","  #### opt0: n2v from graph with id mapping, 64 d\n","  if PSE == 'NoPE':\n","    emb_dim = 64\n","    emb_file = f'{emb_folder}/node2nev_emb_64_for_PE' # f'{emb_folder}/node2nev_emb_64'\n","    with open(emb_file, 'rb') as f:\n","        emb = pickle.load(f)\n","\n","    pe = None\n","    pe_dim = 0\n","    combine_opt = None\n","\n","  #### opt1: n2v from graph with id mapping + LPE Add\n","  if PSE == 'LPE':\n","    emb_dim = 64\n","\n","    emb_file = f'{emb_folder}/node2nev_emb_64_for_PE'\n","    with open(emb_file, 'rb') as f:\n","        emb = pickle.load(f)\n","\n","    lpe_dim = 64\n","    lpe_file = f'{emb_folder}/LPE.tsv'\n","    lpe = file_to_matrix(lpe_file)\n","\n","    lpe_dim = 64\n","    pe = lpe[:,:lpe_dim]\n","\n","    pe_dim = lpe_dim\n","    combine_opt = \"add\"\n","\n","  # #### opt2: n2v from graph with id mapping + LPE add + GPE concat\n","  if PSE == 'SPE':\n","    emb_dim = 64\n","    emb_file = f'{emb_folder}/node2nev_emb_64_for_PE'\n","    with open(emb_file, 'rb') as f:\n","        emb = pickle.load(f)\n","\n","    # --add lpe, lpe_dim = 64\n","    lpe_file = f'{emb_folder}/LPE.tsv'\n","    lpe = file_to_matrix(lpe_file)\n","    emb = np.add(emb, lpe)\n","\n","    # --concat gpe, gpe dim: 8\n","    gpe_dim = 8\n","\n","    gpe_file = f'{emb_folder}/{dataset}/GEE_Z_U.tsv'\n","    gpe = file_to_matrix(lpe_file)\n","\n","    pe = gpe[:,:gpe_dim]\n","    pe_dim = gpe_dim\n","    combine_opt = \"concat\"\n","\n","  return emb_dim, pe_dim, emb, pe, combine_opt\n","\n","########################Transformer_ Train\u0026Valid################################\n","##----adopt from TransformerGO and Modifed for our case------------------------#\n","def get_max_len_seq(dataset):\n","    \"\"\"Finds the dis with the most genes and returns the size\"\"\"\n","    batch_features, batch_labels, batch_ids  = zip(*dataset)\n","    # XQ: VisibleDeprecationWarning: \"batch_features\" contains lists or arrays of varying lengths,\n","    # causing NumPy to treat it as a \"ragged\" array, which is deprecated without explicitly specifying dtype=object.\n","    batch_features = np.array(batch_features, dtype=object)\n","\n","\n","    max_len = 0\n","    for i in range(0, batch_features.shape[0]):\n","        max_len = max(max_len, len(batch_features[i][0]), len(batch_features[i][1]))\n","    return max_len\n","\n","def helper_collate(batch):\n","    MAX_LEN_SEQ = get_max_len_seq(batch)\n","    return transformerGO_collate_fn(batch, MAX_LEN_SEQ, EMB_DIM, pytorch_pad = False)\n","\n","def train(model, iterator, optimizer, criterion,  torch_vers = False):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.train()\n","\n","    pred = []\n","    lab = []\n","    for batch in prog_bar(iterator):\n","        optimizer.zero_grad()\n","\n","        #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","        padded_pairs = batch[0].to(device)\n","        labels = batch[1].to(device)\n","        mask = batch[2].to(device)\n","\n","        #split data into disA and disB\n","        disA_batch = padded_pairs[:,0]\n","        disB_batch = padded_pairs[:,1]\n","\n","        #permute the data to fit the pytorch transformer\n","        if torch_vers:\n","            disA_batch = disA_batch.permute(1,0,2)\n","            disB_batch = disB_batch.permute(1,0,2)\n","\n","        predictions = model(disA_batch, disB_batch, mask[:,0], mask[:,1]).squeeze(1)\n","        loss = criterion(predictions, labels)\n","        acc = binary_accuracy(predictions, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        pred = pred + list(predictions.cpu().data.numpy())\n","        lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred)\n","\n","def evaluate(model, iterator, criterion, torch_vers = False):\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    pred = []\n","    lab = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","\n","            #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","            padded_pairs = batch[0].to(device)\n","            labels = batch[1].to(device)\n","            mask = batch[2].to(device)\n","\n","            #split data into disA and disB\n","            disA_batch = padded_pairs[:,0]\n","            disB_batch = padded_pairs[:,1]\n","\n","            #permute the data to fit the pytorch transformer\n","            if torch_vers:\n","                disA_batch = disA_batch.permute(1,0,2)\n","                disB_batch = disB_batch.permute(1,0,2)\n","\n","            predictions = model(disA_batch, disB_batch, mask[:,0], mask[:,1]).squeeze(1)\n","            loss = criterion(predictions, labels)\n","            acc = binary_accuracy(predictions, labels)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","            pred = pred + list(predictions.cpu().data.numpy())\n","            lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred), lab, pred\n"]},{"cell_type":"markdown","id":"D3AXPld2_u2u","metadata":{"id":"D3AXPld2_u2u"},"source":["# Main"]},{"cell_type":"code","execution_count":13,"id":"h56muLpm6brP","metadata":{"executionInfo":{"elapsed":4537,"status":"ok","timestamp":1734020811510,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"h56muLpm6brP"},"outputs":[],"source":["node_file_path = f'{input_folder}/interactom_nodes.txt'   # stores the nodes for the largest connected component in human Interactome\n","train_file_path = f'{input_folder}/{dataset}/train_set.tsv'\n","test_file_path = f'{input_folder}/{dataset}/test_set.tsv'\n","\n","\n","# 1. get graph original nodes\n","node_idx_dict = get_gene_idx_dict_from_file(node_file_path)\n","node_gene_dict = {v:k for k,v in node_idx_dict.items()}\n","\n","# 2. get selected disease pairs\n","train_dis_pairs, train_labels, train_disease_genes_dict = get_disease_sets(train_file_path)\n","test_dis_pairs, test_labels, test_disease_genes_dict = get_disease_sets(test_file_path)\n","\n","# 3. get embedding and positional encoding value based on given PSE\n","emb_dim, pe_dim, emb, pe, combine_opt = getEmbeddingAndVars(PSE)\n","\n","# 4. prep data and split sets\n","train_origin_set = Dataset_torch(train_dis_pairs, train_labels, train_disease_genes_dict, node_idx_dict, emb, pe, combine_opt)\n","ratio = [0.9, 0.1] # [0.8, 0.2]  # [0.9, 0.1]\n","train_set, valid_set = split_train_valid(train_origin_set, ratio)\n","test_set = Dataset_torch(test_dis_pairs, test_labels, test_disease_genes_dict, node_idx_dict, emb,  pe, combine_opt)\n"]},{"cell_type":"markdown","id":"0ePi7_g8qWis","metadata":{"id":"0ePi7_g8qWis"},"source":["## Set Transformer and Parameters"]},{"cell_type":"code","execution_count":14,"id":"4Ii4um0FHDaG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5109,"status":"ok","timestamp":1734020816617,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"},"user_tz":300},"id":"4Ii4um0FHDaG","outputId":"d58137d0-4467-493e-e1dd-fcb4100a399e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device available:  cuda   Tesla T4\n","350529\n","Train set:  8702 \n"," Valid set:  967 \n"," Test set:  1074 \n","\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-14-c49e5489f114\u003e:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n"]}],"source":["params = {'batch_size': 20,'collate_fn': helper_collate}\n","if pe_dim == 64:\n","  EMB_DIM = emb_dim # (N2V + LPE)\n","else: EMB_DIM = emb_dim + pe_dim  # (N2V + LPE) concat GPE\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device available: \", device, \" \", torch.cuda.get_device_name(0))\n","\n","MODEL_SIZE = EMB_DIM\n","NR_HEADS = 8\n","NR_LAYERS = 3\n","DROPOUT = 0.2\n","SIZE_FF = 4 * MODEL_SIZE\n","LR = 0.0001\n","\n","# if see AssertionError, this is because the dimension (EMB_DIM) cannot divide the headnumber (NR_HEADS)\n","# for the current one, add 8 dim for lpe and keep original 64 dims\n","model = TransformerDis(MODEL_SIZE, NR_HEADS, NR_LAYERS, SIZE_FF, DROPOUT)\n","\n","model = model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.BCEWithLogitsLoss().to(device) # this is the activation function used\n","\n","pytorch_total_params = sum(p.numel() for p in model.parameters())\n","print(pytorch_total_params)\n","\n","# Clear memory and set environment variables\n","torch.cuda.empty_cache()\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","# Enable mixed precision training\n","from torch.cuda.amp import GradScaler, autocast\n","\n","scaler = GradScaler()\n","\n","\n","model_name = f\"{PSE}model{dataset}.pt\"\n","\n","print(\"Train set: \", len(train_set), '\\n', \"Valid set: \", len(valid_set), '\\n', \"Test set: \", len(test_set), '\\n')\n","train_grt = data.DataLoader(train_set, **params, shuffle = True)\n","val_grt = data.DataLoader(valid_set, **params, shuffle = True)\n","test_grt = data.DataLoader(test_set, **params, shuffle = False)\n"]},{"cell_type":"markdown","id":"u1q1sjZqqxvG","metadata":{"id":"u1q1sjZqqxvG"},"source":["## Run the Model"]},{"cell_type":"code","execution_count":null,"id":"p-oCKqUA1yrS","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"p-oCKqUA1yrS"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:12\u003c00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 01 | Epoch Time: 4m 30s \tTrain Loss: 0.681 | Train Acc: 57.98% \t Val. Loss: 0.670 |  Val. Acc: 60.32% \t Roc Train: 0.509 \t Roc Valid: 0.551 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:02\u003c00:00,  1.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 02 | Epoch Time: 4m 19s \tTrain Loss: 0.676 | Train Acc: 58.29% \t Val. Loss: 0.666 |  Val. Acc: 63.00% \t Roc Train: 0.551 \t Roc Valid: 0.624 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:13\u003c00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 03 | Epoch Time: 4m 33s \tTrain Loss: 0.662 | Train Acc: 60.24% \t Val. Loss: 0.650 |  Val. Acc: 61.90% \t Roc Train: 0.615 \t Roc Valid: 0.652 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:03\u003c00:00,  1.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 04 | Epoch Time: 4m 23s \tTrain Loss: 0.643 | Train Acc: 63.22% \t Val. Loss: 0.629 |  Val. Acc: 65.45% \t Roc Train: 0.660 \t Roc Valid: 0.682 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:08\u003c00:00,  1.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 05 | Epoch Time: 4m 29s \tTrain Loss: 0.629 | Train Acc: 66.03% \t Val. Loss: 0.612 |  Val. Acc: 66.65% \t Roc Train: 0.693 \t Roc Valid: 0.703 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:14\u003c00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 06 | Epoch Time: 4m 33s \tTrain Loss: 0.615 | Train Acc: 66.59% \t Val. Loss: 0.611 |  Val. Acc: 65.04% \t Roc Train: 0.709 \t Roc Valid: 0.718 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:14\u003c00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 07 | Epoch Time: 4m 35s \tTrain Loss: 0.599 | Train Acc: 68.12% \t Val. Loss: 0.593 |  Val. Acc: 69.21% \t Roc Train: 0.731 \t Roc Valid: 0.741 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:16\u003c00:00,  1.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 08 | Epoch Time: 4m 35s \tTrain Loss: 0.587 | Train Acc: 69.59% \t Val. Loss: 0.583 |  Val. Acc: 69.83% \t Roc Train: 0.746 \t Roc Valid: 0.744 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:15\u003c00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 09 | Epoch Time: 4m 35s \tTrain Loss: 0.581 | Train Acc: 70.23% \t Val. Loss: 0.561 |  Val. Acc: 70.64% \t Roc Train: 0.754 \t Roc Valid: 0.768 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:07\u003c00:00,  1.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 10 | Epoch Time: 4m 26s \tTrain Loss: 0.571 | Train Acc: 71.34% \t Val. Loss: 0.568 |  Val. Acc: 70.74% \t Roc Train: 0.764 \t Roc Valid: 0.767 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:05\u003c00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 11 | Epoch Time: 4m 25s \tTrain Loss: 0.562 | Train Acc: 72.02% \t Val. Loss: 0.555 |  Val. Acc: 71.17% \t Roc Train: 0.772 \t Roc Valid: 0.778 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:13\u003c00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 12 | Epoch Time: 4m 32s \tTrain Loss: 0.559 | Train Acc: 71.74% \t Val. Loss: 0.551 |  Val. Acc: 71.66% \t Roc Train: 0.776 \t Roc Valid: 0.777 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:10\u003c00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 13 | Epoch Time: 4m 28s \tTrain Loss: 0.551 | Train Acc: 72.17% \t Val. Loss: 0.569 |  Val. Acc: 70.25% \t Roc Train: 0.783 \t Roc Valid: 0.774 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:12\u003c00:00,  1.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 14 | Epoch Time: 4m 33s \tTrain Loss: 0.549 | Train Acc: 72.87% \t Val. Loss: 0.564 |  Val. Acc: 70.20% \t Roc Train: 0.786 \t Roc Valid: 0.777 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:09\u003c00:00,  1.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 15 | Epoch Time: 4m 25s \tTrain Loss: 0.545 | Train Acc: 72.78% \t Val. Loss: 0.542 |  Val. Acc: 70.64% \t Roc Train: 0.789 \t Roc Valid: 0.787 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:10\u003c00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 16 | Epoch Time: 4m 27s \tTrain Loss: 0.538 | Train Acc: 72.82% \t Val. Loss: 0.542 |  Val. Acc: 72.97% \t Roc Train: 0.797 \t Roc Valid: 0.789 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:09\u003c00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 17 | Epoch Time: 4m 27s \tTrain Loss: 0.534 | Train Acc: 73.22% \t Val. Loss: 0.528 |  Val. Acc: 72.27% \t Roc Train: 0.800 \t Roc Valid: 0.798 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:05\u003c00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 18 | Epoch Time: 4m 25s \tTrain Loss: 0.531 | Train Acc: 73.21% \t Val. Loss: 0.536 |  Val. Acc: 72.35% \t Roc Train: 0.802 \t Roc Valid: 0.788 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:11\u003c00:00,  1.73it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 19 | Epoch Time: 4m 30s \tTrain Loss: 0.530 | Train Acc: 73.00% \t Val. Loss: 0.529 |  Val. Acc: 73.09% \t Roc Train: 0.805 \t Roc Valid: 0.799 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:10\u003c00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 20 | Epoch Time: 4m 29s \tTrain Loss: 0.525 | Train Acc: 73.67% \t Val. Loss: 0.544 |  Val. Acc: 72.71% \t Roc Train: 0.807 \t Roc Valid: 0.794 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:09\u003c00:00,  1.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 21 | Epoch Time: 4m 29s \tTrain Loss: 0.523 | Train Acc: 73.60% \t Val. Loss: 0.531 |  Val. Acc: 73.79% \t Roc Train: 0.809 \t Roc Valid: 0.797 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:13\u003c00:00,  1.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 22 | Epoch Time: 4m 33s \tTrain Loss: 0.522 | Train Acc: 73.61% \t Val. Loss: 0.519 |  Val. Acc: 72.29% \t Roc Train: 0.812 \t Roc Valid: 0.806 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:15\u003c00:00,  1.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 23 | Epoch Time: 4m 35s \tTrain Loss: 0.518 | Train Acc: 74.06% \t Val. Loss: 0.540 |  Val. Acc: 71.85% \t Roc Train: 0.814 \t Roc Valid: 0.797 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:05\u003c00:00,  1.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 24 | Epoch Time: 4m 24s \tTrain Loss: 0.513 | Train Acc: 73.94% \t Val. Loss: 0.532 |  Val. Acc: 72.80% \t Roc Train: 0.818 \t Roc Valid: 0.809 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:10\u003c00:00,  1.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 25 | Epoch Time: 4m 29s \tTrain Loss: 0.511 | Train Acc: 74.06% \t Val. Loss: 0.541 |  Val. Acc: 72.07% \t Roc Train: 0.818 \t Roc Valid: 0.800 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:08\u003c00:00,  1.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 26 | Epoch Time: 4m 27s \tTrain Loss: 0.511 | Train Acc: 74.44% \t Val. Loss: 0.521 |  Val. Acc: 73.50% \t Roc Train: 0.819 \t Roc Valid: 0.807 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:04\u003c00:00,  1.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 27 | Epoch Time: 4m 21s \tTrain Loss: 0.506 | Train Acc: 74.45% \t Val. Loss: 0.521 |  Val. Acc: 73.29% \t Roc Train: 0.824 \t Roc Valid: 0.807 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:15\u003c00:00,  1.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 28 | Epoch Time: 4m 35s \tTrain Loss: 0.506 | Train Acc: 74.92% \t Val. Loss: 0.528 |  Val. Acc: 73.29% \t Roc Train: 0.826 \t Roc Valid: 0.804 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:29\u003c00:00,  1.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 29 | Epoch Time: 4m 48s \tTrain Loss: 0.496 | Train Acc: 75.48% \t Val. Loss: 0.509 |  Val. Acc: 73.99% \t Roc Train: 0.832 \t Roc Valid: 0.814 ,   0.0001 --LR\r"]},{"name":"stderr","output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436/436 [04:26\u003c00:00,  1.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 30 | Epoch Time: 4m 45s \tTrain Loss: 0.496 | Train Acc: 74.93% \t Val. Loss: 0.517 |  Val. Acc: 72.78% \t Roc Train: 0.830 \t Roc Valid: 0.820 ,   0.0001 --LR\r"]}],"source":["writer = SummaryWriter(flush_secs=14)\n","N_EPOCHS = 30\n","best_roc_val = float('-inf')\n","\n","\n","for epoch in range(N_EPOCHS):\n","    start_time = time.time()\n","    train_loss, train_acc, roc_train = train(model, train_grt, optimizer, criterion, torch_vers = False)\n","    valid_loss, valid_acc, roc_val, _, _ = evaluate(model, val_grt, criterion, torch_vers = False)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if best_roc_val \u003c roc_val:\n","        best_roc_val = roc_val\n","        torch.save(model.state_dict(),  model_name)\n","\n","    print_status(epoch, epoch_mins, epoch_secs, train_loss,\\\n","                 train_acc, valid_loss, valid_acc, roc_train, roc_val, optimizer)\n","    write_scalars_tensorboard(writer, train_loss, valid_loss, train_acc, valid_acc, epoch)\n"]},{"cell_type":"code","execution_count":null,"id":"AEXD_kWo-yP8","metadata":{"id":"AEXD_kWo-yP8"},"outputs":[],"source":["#WRiTING THE PERFORMANCE ON THE TEST SET #\n","\n","model = TransformerDis(MODEL_SIZE, NR_HEADS, NR_LAYERS, SIZE_FF, DROPOUT)\n","\n","model.load_state_dict(torch.load(model_name))\n","model = model.to(device)\n","\n","EMB_method = 'N2V'\n","\n","with open(f\"{dataset}_{EMB_method}_{PSE}_training-results.txt\", \"a\") as myfile:\n","    myfile.write(f\"\\n ### {model_name} ### \\n\")\n","    myfile.write(f\"\\n ### EMB: {EMB_method}; DIM: {EMB_DIM} ### \\n\")\n","    myfile.write(f\"Train set: {len(train_set)}, Valid set: {len(valid_set)}, Test set: {len(test_set)} \\n\")\n","\n","    valid_loss, valid_acc, roc_val, lab, pred = evaluate(model, test_grt, criterion, torch_vers = False)\n","    myfile.write(f\" \\n valid_loss: {valid_loss}, valid_acc: {valid_acc}, roc_val: {roc_val} \\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["eyQbCqXNAIP8","OZ09R7tlAK4g","0quiQZnHBMnE"],"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}