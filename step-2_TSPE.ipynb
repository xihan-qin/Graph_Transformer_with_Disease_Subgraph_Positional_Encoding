{"cells":[{"cell_type":"markdown","id":"XMZ06nZzOEku","metadata":{"id":"XMZ06nZzOEku"},"source":["# Select GPU"]},{"cell_type":"markdown","id":"wnx5TUIEOMXX","metadata":{"id":"wnx5TUIEOMXX"},"source":["Runtime -> Change Runtime type -> T4 GPU"]},{"cell_type":"markdown","id":"JHNfn4MVN9Fy","metadata":{"id":"JHNfn4MVN9Fy"},"source":["# Check nvidia"]},{"cell_type":"markdown","id":"eyQbCqXNAIP8","metadata":{"id":"eyQbCqXNAIP8"},"source":["## install if needed"]},{"cell_type":"code","execution_count":null,"id":"KBm49WjMX3FG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBm49WjMX3FG","outputId":"f00f737f-1e46-448c-b396-2cc97ab42c5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nvidia-pyindex\n","  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: nvidia-pyindex\n","  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8419 sha256=9c77ce6b59f64da924cb407482ae7f84ba351c73c8f2edc6c8ad4efc81f532f6\n","  Stored in directory: /root/.cache/pip/wheels/2c/af/d0/7a12f82cab69f65d51107f48bcd6179e29b9a69a90546332b3\n","Successfully built nvidia-pyindex\n","Installing collected packages: nvidia-pyindex\n","Successfully installed nvidia-pyindex-1.0.9\n"]}],"source":["!pip install --upgrade nvidia-pyindex"]},{"cell_type":"markdown","id":"OZ09R7tlAK4g","metadata":{"id":"OZ09R7tlAK4g"},"source":["## check"]},{"cell_type":"code","execution_count":null,"id":"yGG2Z3u4XSIb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGG2Z3u4XSIb","outputId":"ceb35978-fbde-4657-c54c-451d1a161904"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Jun 17 05:17:23 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","id":"Q6Vf5O0JOu31","metadata":{"id":"Q6Vf5O0JOu31"},"source":["# Packages"]},{"cell_type":"markdown","id":"GeVa1Pf_0lAa","metadata":{"id":"GeVa1Pf_0lAa"},"source":["## Installation"]},{"cell_type":"markdown","id":"MD11y7kziqwO","metadata":{"id":"MD11y7kziqwO"},"source":["### For numpy version issue, no need if this issue is solved"]},{"cell_type":"code","execution_count":null,"id":"SXms95Q1iV8_","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"SXms95Q1iV8_","outputId":"aa2fefff-9747-4c75-9adc-57576a0e9091","executionInfo":{"status":"ok","timestamp":1719626457713,"user_tz":240,"elapsed":25502,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.23.1\n","  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n","pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.1 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"551e0872e49f4c98a960a01f323d5080"}},"metadata":{}}],"source":["!pip install numpy==1.23.1"]},{"cell_type":"code","execution_count":null,"id":"s0D3ooJ2ihWv","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"s0D3ooJ2ihWv","outputId":"b34d7dd7-4b48-42f6-e0ca-82d36e8c0be1","executionInfo":{"status":"ok","timestamp":1719626493608,"user_tz":240,"elapsed":446,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.23.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["import numpy as np\n","np.version.version"]},{"cell_type":"markdown","id":"nNlCo9yxixGo","metadata":{"id":"nNlCo9yxixGo"},"source":["### For notebook import"]},{"cell_type":"code","execution_count":null,"id":"n6FrARnyFiUg","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n6FrARnyFiUg","outputId":"fa75b1d2-909b-4556-bdb8-67c466efb03c","executionInfo":{"status":"ok","timestamp":1719626509981,"user_tz":240,"elapsed":14329,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting import_ipynb\n","  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (7.34.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import_ipynb) (5.10.4)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (67.7.2)\n","Collecting jedi>=0.16 (from IPython->import_ipynb)\n","  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (3.0.47)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import_ipynb) (4.9.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (4.19.2)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat->import_ipynb) (5.7.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.4)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.1)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import_ipynb) (4.2.2)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import_ipynb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import_ipynb) (0.2.13)\n","Installing collected packages: jedi, import_ipynb\n","Successfully installed import_ipynb-0.1.4 jedi-0.19.1\n"]}],"source":["!pip install import_ipynb"]},{"cell_type":"markdown","id":"0quiQZnHBMnE","metadata":{"id":"0quiQZnHBMnE"},"source":["#Mount Drive"]},{"cell_type":"code","execution_count":null,"id":"zLEqL7d4O4em","metadata":{"id":"zLEqL7d4O4em","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719626548412,"user_tz":240,"elapsed":38437,"user":{"displayName":"Xihan Qin","userId":"10869877831647632037"}},"outputId":"73b0da37-bb79-4449-853f-b73932cffd80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# import packages\n","## for mount drive purpose\n","import os\n","from google.colab import drive\n","# mount drive\n","drive.mount('/content/drive/', force_remount=True)\n","os.chdir('/content/drive/My Drive/Colab_Notebooks/TSPE/')"]},{"cell_type":"markdown","id":"2a8qpRit4urW","metadata":{"id":"2a8qpRit4urW"},"source":["# packages"]},{"cell_type":"code","execution_count":null,"id":"QD8XJYEC4FBQ","metadata":{"id":"QD8XJYEC4FBQ"},"outputs":[],"source":["from utils import get_gene_idx_dict_from_file, file_to_matrix\n","import pickle\n","import numpy as np\n","\n","import torch\n","import torch.utils.data as data\n","\n","import import_ipynb\n","from training_helper import transformerGO_collate_fn\n","\n","from torch.utils.tensorboard import SummaryWriter\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn import metrics\n","\n","import numpy as np\n","import time\n","\n","from tqdm import tqdm as prog_bar #The progress bar\n","\n","import import_ipynb\n","from transformerGO import (TransformerGO_Scratch)\n","from training_helper import (print_status, write_scalars_tensorboard,\n","                             binary_accuracy, epoch_time)"]},{"cell_type":"markdown","id":"oC7b3SKbAX7-","metadata":{"id":"oC7b3SKbAX7-"},"source":["# Set info\n"]},{"cell_type":"code","execution_count":null,"id":"ucbM10J-4d_F","metadata":{"id":"ucbM10J-4d_F"},"outputs":[],"source":["input_folder = 'input'\n","dataset = 'RR0'   # 'RR1'\n","emb_folder = \"embedding\"\n","\n","# PSE = 'NoPE' # opt0\n","# PSE = 'LPE' # opt1\n","# PSE = 'SPE' # opt2\n","\n","node_file_path = f'{input_folder}/interactom_nodes.txt'   # stores the nodes for the largest connected component in human Interactome\n","train_file_path = f'{input_folder}/{dataset}/train_set.tsv'\n","test_file_path = f'{input_folder}/{dataset}/test_set.tsv'"]},{"cell_type":"markdown","id":"tr6jGxJe4zKu","metadata":{"id":"tr6jGxJe4zKu"},"source":["# Prep Data"]},{"cell_type":"code","execution_count":null,"id":"4IQETJLY4xSh","metadata":{"id":"4IQETJLY4xSh"},"outputs":[],"source":["def get_disease_sets(file_path):\n","    dis_pairs = []   #[(disA, disB), ...]\n","    labels = []      # [label, ...]\n","    disease_genes_dict = {}     #{disease: [gene_1, gene_2, ...]}\n","\n","    f = open(file_path, \"r\")\n","    head = True\n","    for line in f:\n","        if head:\n","            head = False\n","            continue\n","\n","        row = line.strip().split(\"\\t\")\n","        dis_pair, disease_a_genes, disease_b_genes, all_genes, rr = row\n","\n","        disease_a, disease_b = dis_pair.split(\"&\")\n","\n","        dis_pairs.append((disease_a, disease_b))\n","        labels.append(int(rr))\n","\n","        disease_genes_dict[disease_a] = disease_a_genes.split(\",\")\n","        disease_genes_dict[disease_b] = disease_b_genes.split(\",\")\n","\n","\n","    f.close()\n","\n","    return dis_pairs, labels, disease_genes_dict\n","\n","# TransformerGO: (np.array(emb_protA), np.array(emb_protB)), label, ((protA, protein_go_anno[protA]), (protB, protein_go_anno[protB]))\n","# Mine: (np.array(emb_disA), np.array(emb_disB)), label, ((disA, disA_gene_list), (disB, disB_gene_list))\n","\n","class Dataset_torch(torch.utils.data.Dataset):\n","    #Characterizes a dataset for PyTorch\n","    def __init__(self, dis_pairs, labels, dis_genes_dict, node_idx_dict, emb, lpe, combine_opt):\n","        self.dis_pairs = dis_pairs\n","        self.labels = labels\n","        self.dis_genes_dict = dis_genes_dict\n","        self.node_idx_dict = node_idx_dict\n","        self.emb = emb\n","        self.lpe = lpe\n","        self.opt = combine_opt\n","\n","    def __len__(self):\n","        return len(self.dis_pairs)\n","\n","    def __getitem__(self, index):\n","\n","        label = self.labels[index]\n","        disA,disB = self.dis_pairs[index]\n","        gene_lists = [self.dis_genes_dict[disA], self.dis_genes_dict[disB]]\n","        dis_pair_genes = [(disA, gene_lists[0]), (disB, gene_lists[1])] # [(disA, disA_gene_list), (disB, disB_gene_list)]\n","        features = [get_features(gene_list, node_idx_dict, self.emb, self.lpe, self.opt) for gene_list in gene_lists]\n","\n","        return np.array((features, label, dis_pair_genes), dtype=object)\n","\n","#------------------------------------------------------------------------------#\n","def get_features(gene_list, node_idx_dict, emb, lpe, combine_opt):\n","    node_idices = [node_idx_dict[gene] for gene in gene_list if gene in node_idx_dict]\n","    feature_vecs = emb[node_idices, :]\n","    # print(type(feature_vecs))\n","    # print(feature_vecs.shape)\n","    # print(type(lpe[node_idices, :]))\n","    # print(lpe[node_idices, :].shape)\n","    # print(type(np.concatenate(emb[node_idices, :], lpe[node_idices, :], axis=1)))\n","    # print(np.concatenate(emb[node_idices, :], lpe[node_idices, :], axis=1).shape)\n","    if lpe is not None:\n","      if combine_opt == \"add\":\n","        feature_vecs = np.add(emb[node_idices, :], lpe[node_idices, :])\n","      else:\n","        feature_vecs = np.concatenate((emb[node_idices, :], lpe[node_idices, :]), axis=1)\n","\n","    return feature_vecs\n","\n","#------------------------------------------------------------------------------#\n","def split_train_valid(ori_dataset, ratio):\n","  sz = len(ori_dataset)\n","  train_set, valid_set = data.random_split(ori_dataset, [int(ratio[0]*sz), sz - (int(ratio[0]*sz)) ] )\n","\n","  return train_set, valid_set\n","\n","########################TransformerGO###########################################\n","def get_max_len_seq(dataset):\n","    \"\"\"Finds the dis with the most genes and returns the size\"\"\"\n","    batch_features, batch_labels, batch_ids  = zip(*dataset)\n","    batch_features = np.array(batch_features)\n","\n","\n","    max_len = 0\n","    for i in range(0, batch_features.shape[0]):\n","        max_len = max(max_len, len(batch_features[i][0]), len(batch_features[i][1]))\n","    return max_len\n","\n","def helper_collate(batch):\n","    MAX_LEN_SEQ = get_max_len_seq(batch)\n","    return transformerGO_collate_fn(batch, MAX_LEN_SEQ, EMB_DIM, pytorch_pad = False)\n","\n","def train(model, iterator, optimizer, criterion,  torch_vers = False):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    model.train()\n","\n","    pred = []\n","    lab = []\n","    for batch in prog_bar(iterator):\n","        optimizer.zero_grad()\n","\n","        #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","        padded_pairs = batch[0].to(device)\n","        labels = batch[1].to(device)\n","        mask = batch[2].to(device)\n","\n","        #split data into protA and protB\n","        gosetA_batch = padded_pairs[:,0]\n","        gosetB_batch = padded_pairs[:,1]\n","\n","        #permute the data to fit the pytorch transformer\n","        if torch_vers:\n","            gosetA_batch = gosetA_batch.permute(1,0,2)\n","            gosetB_batch = gosetB_batch.permute(1,0,2)\n","\n","        predictions = model(gosetA_batch, gosetB_batch, mask[:,0], mask[:,1]).squeeze(1)\n","        loss = criterion(predictions, labels)\n","        acc = binary_accuracy(predictions, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        pred = pred + list(predictions.cpu().data.numpy())\n","        lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred)\n","\n","def evaluate(model, iterator, criterion, torch_vers = False):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    pred = []\n","    lab = []\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for batch in iterator:\n","\n","            #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","            padded_pairs = batch[0].to(device)\n","            labels = batch[1].to(device)\n","            mask = batch[2].to(device)\n","\n","            #split data into protA and protB\n","            gosetA_batch = padded_pairs[:,0]\n","            gosetB_batch = padded_pairs[:,1]\n","\n","            #permute the data to fit the pytorch transformer\n","            if torch_vers:\n","                gosetA_batch = gosetA_batch.permute(1,0,2)\n","                gosetB_batch = gosetB_batch.permute(1,0,2)\n","\n","            predictions = model(gosetA_batch, gosetB_batch, mask[:,0], mask[:,1]).squeeze(1)\n","            loss = criterion(predictions, labels)\n","            acc = binary_accuracy(predictions, labels)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","            pred = pred + list(predictions.cpu().data.numpy())\n","            lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), roc_auc_score(lab,pred), lab, pred\n","\n","def evaluate_cosine(iterator, criterion, torch_vers = False):\n","\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    pred = []\n","    lab = []\n","\n","    for batch in prog_bar(iterator):\n","\n","        #padded pairs: tensor of shape N * 2(protein pair) * L(longest seq) * Emb dim\n","        padded_pairs = batch[0].to(device)\n","        labels = batch[1].to(device)\n","\n","        #split data into protA and protB\n","        gosetA_batch = padded_pairs[:,0]\n","        gosetB_batch = padded_pairs[:,1]\n","\n","        vector_protA = torch.sum(gosetA_batch, dim = 1)\n","        vector_protB = torch.sum(gosetB_batch, dim = 1)\n","\n","        cosine = nn.CosineSimilarity(dim=1, eps=1e-6)\n","        predictions = cosine(vector_protA, vector_protB)\n","        loss = criterion(predictions, labels)\n","        acc = binary_accuracy(predictions, labels)\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","        pred = pred + list(predictions.cpu().data.numpy())\n","        lab = lab + list(labels.cpu().data.numpy())\n","\n","    return epoch_loss / len(iterator), epoch_acc / len(iterator), lab, pred"]},{"cell_type":"code","execution_count":null,"id":"h56muLpm6brP","metadata":{"id":"h56muLpm6brP"},"outputs":[],"source":["# 1. get graph original nodes\n","node_idx_dict = get_gene_idx_dict_from_file(node_file_path)\n","node_gene_dict = {v:k for k,v in node_idx_dict.items()}\n","\n","# 2. get selected disease pairs\n","train_dis_pairs, train_labels, train_disease_genes_dict = get_disease_sets(train_file_path)\n","test_dis_pairs, test_labels, test_disease_genes_dict = get_disease_sets(test_file_path)\n","\n","# 3. get embedding file\n","#### opt0: n2v from graph with id mapping, 64 d\n","if PSE == 'NoPE':\n","  emb_dim = 64\n","  lpe_dim = 0\n","  emb_file = f'{emb_folder}/node2nev_emb_64' # f'{emb_folder}/node2nev_emb_64'\n","  with open(emb_file, 'rb') as f:\n","      emb = pickle.load(f)\n","  lpe = None\n","  combine_opt = None\n","\n","#### opt1: n2v from graph with id mapping + LPE Add\n","if PSE == 'LPE':\n","  emb_dim = 64\n","  lpe_dim = 64\n","  emb_file = f'{emb_folder}/node2nev_emb_64'\n","  with open(emb_file, 'rb') as f:\n","      emb = pickle.load(f)\n","\n","  lpe_file = f'{emb_folder}/LPE.tsv'\n","  lpe = file_to_matrix(lpe_file)\n","  combine_opt = \"add\"\n","\n","#### opt3: n2v from graph with id mapping + LPE add + GPE concat\n","if PSE == 'SPE':\n","  emb_dim = 64\n","  lpe_dim = 64\n","\n","  emb_file = f'{emb_folder}/node2nev_emb_64'\n","  with open(emb_file, 'rb') as f:\n","      emb = pickle.load(f)\n","\n","  # --add lpe\n","  lpe_file = f'{emb_folder}/LPE.tsv'\n","  lpe = file_to_matrix(lpe_file)\n","  emb = np.add(emb, lpe)\n","\n","  # --concat Gpe later\n","  lpe_dim = 8\n","\n","  lpe_file = f'{emb_folder}/GPE.tsv'\n","  lpe = file_to_matrix(lpe_file)\n","  lpe = lpe[:,1:lpe_dim+1]\n","  combine_opt = \"concat\"\n","\n","\n","train_origin_set = Dataset_torch(train_dis_pairs, train_labels, train_disease_genes_dict, node_idx_dict, emb, lpe, combine_opt)\n","ratio = [0.9, 0.1] # [0.8, 0.2]  # [0.9, 0.1]\n","train_set, valid_set = split_train_valid(train_origin_set, ratio)\n","test_set = Dataset_torch(test_dis_pairs, test_labels, test_disease_genes_dict, node_idx_dict, emb,  lpe, combine_opt)\n","\n","params = {'batch_size': 20,'collate_fn': helper_collate}\n","\n","print(\"Train set: \", len(train_set), '\\n', \"Valid set: \", len(valid_set), '\\n', \"Test set: \", len(test_set), '\\n')\n","train_grt = data.DataLoader(train_set, **params, shuffle = True)\n","val_grt = data.DataLoader(valid_set, **params, shuffle = True)\n","test_grt = data.DataLoader(test_set, **params, shuffle = False)\n","\n","\n","if lpe_dim == 64:\n","  EMB_DIM = emb_dim\n","else: EMB_DIM = emb_dim + lpe_dim  # N2V concate LPE, 64 *2 dimensions too much, cannot run\n","\n","print(f\"emb dim: {EMB_DIM}\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device available: \", device, \" \", torch.cuda.get_device_name(0))\n","\n","#TRANSFORMERGO MODEL #\n","MODEL_SIZE = EMB_DIM\n","NR_HEADS = 8\n","NR_LAYERS = 3\n","DROPOUT = 0.2\n","SIZE_FF = 4 * MODEL_SIZE\n","LR = 0.0001\n","\n","# if see AssertionError, this is because the dimension (EMB_DIM) cannot divide the headnumber (NR_HEADS)\n","# for the current one, add 8 dim for gpe and keep original 64 dims\n","model = TransformerGO_Scratch(MODEL_SIZE, NR_HEADS, NR_LAYERS, SIZE_FF, DROPOUT)\n","\n","model = model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=LR)\n","criterion = nn.BCEWithLogitsLoss().to(device) # this is the activation function used\n","\n","pytorch_total_params = sum(p.numel() for p in model.parameters())\n","print(pytorch_total_params)\n"]},{"cell_type":"code","source":["model_name = \"model.pt\"\n","\n","writer = SummaryWriter(flush_secs=14)\n","N_EPOCHS = 30\n","best_roc_val = float('-inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","\n","    start_time = time.time()\n","    train_loss, train_acc, roc_train = train(model, train_grt, optimizer, criterion, torch_vers = False)\n","    valid_loss, valid_acc, roc_val, _, _ = evaluate(model, val_grt, criterion, torch_vers = False)\n","    end_time = time.time()\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if best_roc_val < roc_val:\n","        best_roc_val = roc_val\n","        torch.save(model.state_dict(),  model_name)\n","\n","    print_status(epoch, epoch_mins, epoch_secs, train_loss,\\\n","                 train_acc, valid_loss, valid_acc, roc_train, roc_val, optimizer)\n","    write_scalars_tensorboard(writer, train_loss, valid_loss, train_acc, valid_acc, epoch)"],"metadata":{"id":"p-oCKqUA1yrS"},"id":"p-oCKqUA1yrS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"AEXD_kWo-yP8","metadata":{"id":"AEXD_kWo-yP8"},"outputs":[],"source":["#WRiTING THE PERFORMANCE ON THE TEST SET #\n","model = TransformerGO_Scratch(MODEL_SIZE, NR_HEADS, NR_LAYERS, SIZE_FF, DROPOUT)\n","model.load_state_dict(torch.load(model_name))\n","model = model.to(device)\n","\n","EMB_method = 'N2V'\n","\n","with open(f\"{dataset}_{fold}_{EMB_method}_{PSE}_training-results.txt\", \"a\") as myfile:\n","    myfile.write(f\"\\n ### {model_name} ### \\n\")\n","    myfile.write(f\"\\n ### EMB: {EMB_method}; DIM: {EMB_DIM} ### \\n\")\n","    myfile.write(f\"Train set: {len(train_set)}, Valid set: {len(valid_set)}, Test set: {len(test_set)} \\n\")\n","\n","    valid_loss, valid_acc, roc_val, lab, pred = evaluate(model, test_grt, criterion, torch_vers = False)\n","    myfile.write(f\" \\n valid_loss: {valid_loss}, valid_acc: {valid_acc}, roc_val: {roc_val} \\n\")"]},{"cell_type":"markdown","id":"_kZEtDJm_jVm","metadata":{"id":"_kZEtDJm_jVm"},"source":["# Disconnect when done"]},{"cell_type":"code","execution_count":null,"id":"LIGxgxfT_eOs","metadata":{"id":"LIGxgxfT_eOs"},"outputs":[],"source":["!kill $(ps aux | awk '{print $2}')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["JHNfn4MVN9Fy","eyQbCqXNAIP8","OZ09R7tlAK4g","GeVa1Pf_0lAa","MD11y7kziqwO","nNlCo9yxixGo","0quiQZnHBMnE"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}